import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Support Vector Machine": SVC(),
    "K-Nearest Neighbors": KNeighborsClassifier()
}




st.title('Ethical AI Detector')
st.write('Welcome to the Ethical AI Bias Detector â€“ upload a dataset and analyze it for bias and variance.')

uploaded_file = st.file_uploader("Upload your CSV dataset",type=["csv"])

if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)
    
    st.subheader("ðŸ“Š Dataset Preview")
    st.write(df.head())
    
    st.write(f" Rows: {df.shape[0]} | Columns: {df.shape[1]}")
    
    st.subheader(" Missing Values")
    st.write(df.isnull().sum())
    
    st.subheader(" Column Types")
    st.write(df.dtypes)
    
    st.subheader("Select Target Column")
    target_column = st.selectbox("Choose the target (label) column", df.columns)
    
    if df[target_column].nunique() > 10:
        st.warning("âš ï¸ Your target column seems continuous. Please choose a classification column (e.g. yes/no, 0/1).")


    st.subheader(" Select Sensitive Column")
    sensitive_column = st.selectbox("Choose the sensitive attribute", df.columns)

    st.success(f"Target: {target_column} | Sensitive Feature: {sensitive_column}")
    
    
    st.subheader("ðŸ¤– Choose Model for Training")
    model_name = st.selectbox("Select ML Model", [
        "Logistic Regression", 
        "Decision Tree", 
        "Random Forest", 
        "Support Vector Machine", 
        "K-Nearest Neighbors"
        ])

    if(model_name=='Logistic Regression'):
            model = LogisticRegression(max_iter=1000)
    elif(model_name=='Decision Tree'):
            model = DecisionTreeClassifier()
    elif(model_name=='Random Forest'):
            model = RandomForestClassifier()
    elif(model_name=='Support Vector Machine'):
            model = SVC()
    elif(model_name=='K-Nearest Neighbors'):
            model = KNeighborsClassifier()

    y = df[target_column]
    sensitive = df[sensitive_column]

    # 2. Drop target and sensitive column to get features
    X = df.drop([target_column, sensitive_column], axis=1)

    # 3. Convert categorical features to numeric
    X = pd.get_dummies(X, drop_first=True)

    # 4. Encode sensitive column if it's categorical (for grouping)
    if sensitive.dtype == 'object':
         sensitive = sensitive.astype('category').cat.codes
        
    st.success("âœ… Data preprocessed successfully! Ready for model training.")
        
    X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(
            X, y, sensitive, test_size=0.2, random_state=42
        )

    X_test = X_test.reset_index(drop=True)
    y_test = y_test.reset_index(drop=True)
    sensitive_test = sensitive_test.reset_index(drop=True)

    if st.button("Train and Analyze Bias"):
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        overall_accuracy = accuracy_score(y_test, y_pred)
        
        X_test[sensitive_column] = sensitive_test

        group_accuracies = []
        for group in X_test[sensitive_column].unique():
            idx = X_test[X_test[sensitive_column] == group].index
            acc = accuracy_score(y_test.loc[idx], y_pred[idx])
            group_accuracies.append((group, acc))

     
        accuracies_only = [acc for _, acc in group_accuracies]
        bias_gap = max(accuracies_only) - min(accuracies_only)
        variance = np.var(accuracies_only)

        st.subheader("ðŸ“‰ Bias & Variance Summary")

        st.write(f"ðŸŽ¯ Overall Accuracy: `{accuracy_score(y_test, y_pred):.2f}`")
        st.write(f"ðŸ”º **Bias Gap (Max - Min Accuracy):** `{bias_gap:.2f}`")
        st.write(f"ðŸ“ˆ **Variance Across Groups:** `{variance:.4f}`")

        if bias_gap > 0.2:
            st.error("âš ï¸ High bias detected! Consider mitigation strategies.")
        elif bias_gap > 0.1:
            st.warning("âš ï¸ Moderate bias detected.")
        else:
            st.success("âœ… Low bias. Model is performing fairly across groups.")
            
            
        if variance > 0.02:
            st.error("âš ï¸ High variance across groups â€“ model performance is inconsistent.")
        elif variance > 0.01:
            st.warning("âš ï¸ Moderate variance across groups.")
        else:
            st.success("âœ… Low variance â€“ model is consistent across groups.")
            
        if bias_gap > 0.2 or variance > 0.02:
            st.subheader("ðŸ› ï¸ Bias Mitigation Suggestions")
            st.markdown("""
        - **Re-sampling**: Try oversampling underrepresented groups (like using SMOTE) or undersampling the dominant group.
        - **Reweighting**: Assign different weights to samples from different sensitive groups during model training.
        - **Fair Classifiers**: Use algorithms like `fairlearn`, `aif360`, or `fairboost` that build fairness directly into models.
        - **Feature Removal**: Remove sensitive attributes or correlated proxies if they leak bias.
        - **Threshold Adjustment**: Post-process predictions and adjust thresholds per group to equalize outcomes.
            """)

            st.info("ðŸ“Œ Tip: You can explore packages like `Fairlearn`, `AIF360`, or `FairXGBoost` to implement these methods.")

        
        top_sensitive_values = df[sensitive_column].value_counts().nlargest(10).index
        filtered_df = df[df[sensitive_column].isin(top_sensitive_values)]

        # Convert target to numeric if needed
        if filtered_df[target_column].dtype == 'object':
            filtered_df[target_column] = filtered_df[target_column].astype('category').cat.codes

        # Group and plot
        filtered_df[sensitive_column] = filtered_df[sensitive_column].apply(lambda x: str(x)[:40] + "..." if len(str(x)) > 40 else str(x))

        group_means = filtered_df.groupby(sensitive_column)[target_column].mean()
        st.subheader("ðŸ“Š Visualization: Sensitive vs Target Column")
        fig, ax = plt.subplots(figsize=(12, 5))
        group_means.plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')
        ax.set_ylabel(f"Mean {target_column}")
        ax.set_title(f"Mean {target_column} by {sensitive_column}")
        ax.tick_params(axis='x', labelrotation=45, labelsize=9)
        
        for i, v in enumerate(group_means):
            ax.text(i, v + 0.02, f"{v:.2f}", ha='center', va='bottom', fontsize=8)
        
        st.pyplot(fig)


    if (st.button("Train and Compare all models")):
        comparison_results=[]
        
        for model_name,model in models.items():
            try:
                model.fit(X_train,y_train)
                y_pred = model.predict(X_test)
                acc = accuracy_score(y_test,y_pred)
                
                group_accuracies=[]
                for group in sensitive_test.unique():
                    idx = sensitive_test[sensitive_test==group].index
                    group_acc = accuracy_score(y_test.iloc[idx],y_pred[idx])
                    group_accuracies.append(group_acc)
                    
                bias_gap = max(group_accuracies)-min(group_accuracies)
                variance = np.var(group_accuracies)
                
                comparison_results.append({
                    "Model":model_name , 
                    "Accuracy":round(acc,3),
                    "Bias Gap":round(bias_gap,3),
                    "Variance":round(variance,3),
                })
                
            except Exception as e:
                st.error(f"Model {model_name} failed: {str(e)}")
                
            
        st.subheader("ðŸ“Š Model Comparison Results")
        result_df = pd.DataFrame(comparison_results)
        st.dataframe(result_df.sort_values(by="Bias Gap", ascending=True).reset_index(drop=True))        
        
        if not result_df.empty:
            best_model = result_df.loc[result_df['Bias Gap'].idxmin()]
            st.success(f"Lowest Bias:'{best_model['Model']}' with Bias Gap = '{best_model['Bias Gap']}'")
            
        st.subheader("ðŸ“Š Visual Comparison of Models")
        fig, ax = plt.subplots(figsize=(10, 5))
        result_df.plot(x="Model", y=["Bias Gap", "Variance"], kind="bar", ax=ax)
        plt.xticks(rotation=45)
        plt.ylabel("Metric Value")
        plt.title("Bias Gap and Variance Comparison Across Models")
        st.pyplot(fig)
